{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from pysmiles import read_smiles\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepchem as dc\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "from deepchem.models import GraphConvModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.trans import NormalizationTransformer\n",
    "from deepchem.splits import IndexSplitter\n",
    "\n",
    "\n",
    "def Splitting_data(dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1):\n",
    "    splitter = IndexSplitter()\n",
    "    train, valid, test = splitter.train_valid_test_split(dataset, frac_train=frac_train, frac_valid=frac_valid, frac_test=frac_test)\n",
    "    train, valid, test = splitter.train_valid_test_split(dataset)\n",
    "\n",
    "    transformers = [NormalizationTransformer(transform_y = True, dataset = train, move_mean = True)]\n",
    "\n",
    "  \n",
    "    for transformer in transformers:\n",
    "        train = transformer.transform(train)\n",
    "        valid = transformer.transform(valid)\n",
    "        test = transformer.transform(test)\n",
    "    \n",
    "    return (train, valid, test), transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(tasks, smiles, dataset_file):\n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    \n",
    "    loader = dc.data.CSVLoader(tasks = tasks, smiles_field = smiles, featurizer=featurizer)\n",
    "    dataset = loader.featurize(dataset_file, shard_size=8192)\n",
    "    data_set, transformers = Splitting_data(dataset)\n",
    "    \n",
    "    return tasks, data_set, transformers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_conv(tasks, dataset, transformers, batch_size = 128):\n",
    "    train_dataset, valid_dataset, test_dataset = dataset\n",
    "\n",
    "    #Fit models\n",
    "    metric = dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)\n",
    "\n",
    "    #Do setup required for tf/keras models\n",
    "    #Number of features on conv-mols\n",
    "    n_feat = 75\n",
    "    #Batch size of models\n",
    "    model = GraphConvModel(len(tasks), batch_size=batch_size, mode='regression')\n",
    "\n",
    "    #Fit trained model\n",
    "    model.fit(train_dataset, nb_epoch=20)\n",
    "\n",
    "    print(\"Evaluating model\")\n",
    "    train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
    "    valid_scores = model.evaluate(valid_dataset, [metric], transformers)\n",
    "    test_scores = model.evaluate(test_dataset, [metric], transformers)\n",
    "\n",
    "    print(\"Train scores\")\n",
    "    print(train_scores)\n",
    "\n",
    "    print(\"Validation scores\")\n",
    "    print(valid_scores)\n",
    "    \n",
    "    print(\"Test scores\")\n",
    "    print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESOL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ESOL/delaney-processed.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 1.350 s\n",
      "TIMING: dataset construction took 1.618 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.328 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.115 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.201 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.280 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.132 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.125 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.233 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.032 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.033 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "esol_labels, esol_dataset, esol_trans = data_load(['measured log solubility in mols per litre'], 'smiles', 'ESOL/delaney-processed.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.unsorted_segment_max is deprecated. Please use tf.math.unsorted_segment_max instead.\n",
      "\n",
      "Help on GraphConvModel in module deepchem.models.graph_models object:\n",
      "\n",
      "class GraphConvModel(deepchem.models.keras_model.KerasModel)\n",
      " |  GraphConvModel(n_tasks, graph_conv_layers=[64, 64], dense_layer_size=128, dropout=0.0, mode='classification', number_atom_features=75, n_classes=2, uncertainty=False, batch_size=100, **kwargs)\n",
      " |  \n",
      " |  This is a DeepChem model implemented by a Keras model.\n",
      " |  \n",
      " |  This class provides several advantages over using the Keras model's fitting\n",
      " |  and prediction methods directly.\n",
      " |  \n",
      " |  1. It provides better integration with the rest of DeepChem, such as direct\n",
      " |     support for Datasets and Transformers.\n",
      " |  \n",
      " |  2. It defines the loss in a more flexible way.  In particular, Keras does not\n",
      " |     support multidimensional weight matrices, which makes it impossible to\n",
      " |     implement most multitask models with Keras.\n",
      " |  \n",
      " |  3. It provides various additional features not found in the Keras Model class,\n",
      " |     such as uncertainty prediction and saliency mapping.\n",
      " |  \n",
      " |  The loss function for a model can be defined in two different ways.  For\n",
      " |  models that have only a single output and use a standard loss function, you\n",
      " |  can simply provide a dc.models.losses.Loss object.  This defines the loss for\n",
      " |  each sample or sample/task pair.  The result is automatically multiplied by\n",
      " |  the weights and averaged over the batch.  Any additional losses computed by\n",
      " |  model layers, such as weight decay penalties, are also added.\n",
      " |  \n",
      " |  For more complicated cases, you can instead provide a function that directly\n",
      " |  computes the total loss.  It must be of the form f(outputs, labels, weights),\n",
      " |  taking the list of outputs from the model, the expected values, and any weight\n",
      " |  matrices.  It should return a scalar equal to the value of the loss function\n",
      " |  for the batch.  No additional processing is done to the result; it is up to\n",
      " |  you to do any weighting, averaging, adding of penalty terms, etc.\n",
      " |  \n",
      " |  You can optionally provide an output_types argument, which describes how to\n",
      " |  interpret the model's outputs.  This should be a list of strings, one for each\n",
      " |  output.  Each entry must have one of the following values:\n",
      " |  \n",
      " |  - 'prediction': This is a normal output, and will be returned by predict().\n",
      " |    If output types are not specified, all outputs are assumed to be of this\n",
      " |    type.\n",
      " |  \n",
      " |  - 'loss': This output will be used in place of the normal outputs for\n",
      " |    computing the loss function.  For example, models that output probability\n",
      " |    distributions usually do it by computing unbounded numbers (the logits),\n",
      " |    then passing them through a softmax function to turn them into\n",
      " |    probabilities.  When computing the cross entropy, it is more numerically\n",
      " |    stable to use the logits directly rather than the probabilities.  You can\n",
      " |    do this by having the model produce both probabilities and logits as\n",
      " |    outputs, then specifying output_types=['prediction', 'loss'].  When\n",
      " |    predict() is called, only the first output (the probabilities) will be\n",
      " |    returned.  But during training, it is the second output (the logits) that\n",
      " |    will be passed to the loss function.\n",
      " |  \n",
      " |  - 'variance': This output is used for estimating the uncertainty in another\n",
      " |    output.  To create a model that can estimate uncertainty, there must be the\n",
      " |    same number of 'prediction' and 'variance' outputs.  Each variance output\n",
      " |    must have the same shape as the corresponding prediction output, and each\n",
      " |    element is an estimate of the variance in the corresponding prediction.\n",
      " |    Also be aware that if a model supports uncertainty, it MUST use dropout on\n",
      " |    every layer, and dropout most be enabled during uncertainty prediction.\n",
      " |    Otherwise, the uncertainties it computes will be inaccurate.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GraphConvModel\n",
      " |      deepchem.models.keras_model.KerasModel\n",
      " |      deepchem.models.models.Model\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_tasks, graph_conv_layers=[64, 64], dense_layer_size=128, dropout=0.0, mode='classification', number_atom_features=75, n_classes=2, uncertainty=False, batch_size=100, **kwargs)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_tasks: int\n",
      " |        Number of tasks\n",
      " |      graph_conv_layers: list of int\n",
      " |        Width of channels for the Graph Convolution Layers\n",
      " |      dense_layer_size: int\n",
      " |        Width of channels for Atom Level Dense Layer before GraphPool\n",
      " |      dropout: list or float\n",
      " |        the dropout probablity to use for each layer.  The length of this list should equal\n",
      " |        len(graph_conv_layers)+1 (one value for each convolution layer, and one for the\n",
      " |        dense layer).  Alternatively this may be a single value instead of a list, in which\n",
      " |        case the same value is used for every layer.\n",
      " |      mode: str\n",
      " |        Either \"classification\" or \"regression\"\n",
      " |      number_atom_features: int\n",
      " |          75 is the default number of atom features created, but\n",
      " |          this can vary if various options are passed to the\n",
      " |          function atom_features in graph_features\n",
      " |      n_classes: int\n",
      " |        the number of classes to predict (only used in classification mode)\n",
      " |      uncertainty: bool\n",
      " |        if True, include extra outputs and loss terms to enable the uncertainty\n",
      " |        in outputs to be predicted\n",
      " |  \n",
      " |  default_generator(self, dataset, epochs=1, mode='fit', deterministic=True, pad_batches=True)\n",
      " |      Create a generator that iterates batches for a dataset.\n",
      " |      \n",
      " |      Subclasses may override this method to customize how model inputs are\n",
      " |      generated from the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: Dataset\n",
      " |        the data to iterate\n",
      " |      epochs: int\n",
      " |        the number of times to iterate over the full dataset\n",
      " |      mode: str\n",
      " |        allowed values are 'fit' (called during training), 'predict' (called\n",
      " |        during prediction), and 'uncertainty' (called during uncertainty\n",
      " |        prediction)\n",
      " |      deterministic: bool\n",
      " |        whether to iterate over the dataset in order, or randomly shuffle the\n",
      " |        data for each epoch\n",
      " |      pad_batches: bool\n",
      " |        whether to pad each batch up to this model's preferred batch size\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a generator that iterates batches, each represented as a tuple of lists:\n",
      " |      ([inputs], [outputs], [weights])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from deepchem.models.keras_model.KerasModel:\n",
      " |  \n",
      " |  compute_saliency(self, X)\n",
      " |      Compute the saliency map for an input sample.\n",
      " |      \n",
      " |      This computes the Jacobian matrix with the derivative of each output element\n",
      " |      with respect to each input element.  More precisely,\n",
      " |      \n",
      " |      - If this model has a single output, it returns a matrix of shape\n",
      " |        (output_shape, input_shape) with the derivatives.\n",
      " |      - If this model has multiple outputs, it returns a list of matrices, one\n",
      " |        for each output.\n",
      " |      \n",
      " |      This method cannot be used on models that take multiple inputs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the input data for a single sample\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      the Jacobian matrix, or a list of matrices\n",
      " |  \n",
      " |  evaluate_generator(self, generator, metrics, transformers=[], per_task_metrics=False)\n",
      " |      Evaluate the performance of this model on the data produced by a generator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generator: generator\n",
      " |        this should generate batches, each represented as a tuple of the form\n",
      " |        (inputs, labels, weights).\n",
      " |      metric: deepchem.metrics.Metric\n",
      " |        Evaluation metric\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      per_task_metrics: bool\n",
      " |        If True, return per-task scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        Maps tasks to scores under metric.\n",
      " |  \n",
      " |  fit(self, dataset, nb_epoch=10, max_checkpoints_to_keep=5, checkpoint_interval=1000, deterministic=False, restore=False, variables=None, loss=None, callbacks=[])\n",
      " |      Train this model on a dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: Dataset\n",
      " |        the Dataset to train on\n",
      " |      nb_epoch: int\n",
      " |        the number of epochs to train for\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      checkpoint_interval: int\n",
      " |        the frequency at which to write checkpoints, measured in training steps.\n",
      " |        Set this to 0 to disable automatic checkpointing.\n",
      " |      deterministic: bool\n",
      " |        if True, the samples are processed in order.  If False, a different random\n",
      " |        order is used for each epoch.\n",
      " |      restore: bool\n",
      " |        if True, restore the model from the most recent checkpoint and continue training\n",
      " |        from there.  If False, retrain the model from scratch.\n",
      " |      variables: list of tf.Variable\n",
      " |        the variables to train.  If None (the default), all trainable variables in\n",
      " |        the model are used.\n",
      " |      loss: function\n",
      " |        a function of the form f(outputs, labels, weights) that computes the loss\n",
      " |        for each batch.  If None (the default), the model's standard loss function\n",
      " |        is used.\n",
      " |      callbacks: function or list of functions\n",
      " |        one or more functions of the form f(model, step) that will be invoked after\n",
      " |        every step.  This can be used to perform validation, logging, etc.\n",
      " |  \n",
      " |  fit_generator(self, generator, max_checkpoints_to_keep=5, checkpoint_interval=1000, restore=False, variables=None, loss=None, callbacks=[])\n",
      " |      Train this model on data from a generator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generator: generator\n",
      " |        this should generate batches, each represented as a tuple of the form\n",
      " |        (inputs, labels, weights).\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      checkpoint_interval: int\n",
      " |        the frequency at which to write checkpoints, measured in training steps.\n",
      " |        Set this to 0 to disable automatic checkpointing.\n",
      " |      restore: bool\n",
      " |        if True, restore the model from the most recent checkpoint and continue training\n",
      " |        from there.  If False, retrain the model from scratch.\n",
      " |      variables: list of tf.Variable\n",
      " |        the variables to train.  If None (the default), all trainable variables in\n",
      " |        the model are used.\n",
      " |      loss: function\n",
      " |        a function of the form f(outputs, labels, weights) that computes the loss\n",
      " |        for each batch.  If None (the default), the model's standard loss function\n",
      " |        is used.\n",
      " |      callbacks: function or list of functions\n",
      " |        one or more functions of the form f(model, step) that will be invoked after\n",
      " |        every step.  This can be used to perform validation, logging, etc.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      the average loss over the most recent checkpoint interval\n",
      " |  \n",
      " |  fit_on_batch(self, X, y, w, variables=None, loss=None, callbacks=[])\n",
      " |      Perform a single step of training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the inputs for the batch\n",
      " |      y: ndarray\n",
      " |        the labels for the batch\n",
      " |      w: ndarray\n",
      " |        the weights for the batch\n",
      " |      variables: list of tf.Variable\n",
      " |        the variables to train.  If None (the default), all trainable variables in\n",
      " |        the model are used.\n",
      " |      loss: function\n",
      " |        a function of the form f(outputs, labels, weights) that computes the loss\n",
      " |        for each batch.  If None (the default), the model's standard loss function\n",
      " |        is used.\n",
      " |      callbacks: function or list of functions\n",
      " |        one or more functions of the form f(model, step) that will be invoked after\n",
      " |        every step.  This can be used to perform validation, logging, etc.\n",
      " |  \n",
      " |  get_checkpoints(self, model_dir=None)\n",
      " |      Get a list of all available checkpoint files.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model_dir: str, default None\n",
      " |        Directory to get list of checkpoints from. Reverts to self.model_dir if None\n",
      " |  \n",
      " |  get_global_step(self)\n",
      " |      Get the number of steps of fitting that have been performed.\n",
      " |  \n",
      " |  load_from_pretrained(self, source_model, assignment_map=None, value_map=None, checkpoint=None, model_dir=None, include_top=True, **kwargs)\n",
      " |      Copies variable values from a pretrained model. `source_model` can either\n",
      " |      be a pretrained model or a model with the same architecture. `value_map`\n",
      " |      is a variable-value dictionary. If no `value_map` is provided, the variable\n",
      " |      values are restored to the `source_model` from a checkpoint and a default\n",
      " |      `value_map` is created. `assignment_map` is a dictionary mapping variables\n",
      " |      from the `source_model` to the current model. If no `assignment_map` is\n",
      " |      provided, one is made from scratch and assumes the model is composed of\n",
      " |      several different layers, with the final one being a dense layer. include_top\n",
      " |      is used to control whether or not the final dense layer is used. The default\n",
      " |      assignment map is useful in cases where the type of task is different\n",
      " |      (classification vs regression) and/or number of tasks in the setting.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source_model: dc.KerasModel, required\n",
      " |        source_model can either be the pretrained model or a dc.KerasModel with\n",
      " |        the same architecture as the pretrained model. It is used to restore from\n",
      " |        a checkpoint, if value_map is None and to create a default assignment map\n",
      " |        if assignment_map is None\n",
      " |      assignment_map: Dict, default None\n",
      " |        Dictionary mapping the source_model variables and current model variables\n",
      " |      value_map: Dict, default None\n",
      " |        Dictionary containing source_model trainable variables mapped to numpy\n",
      " |        arrays. If value_map is None, the values are restored and a default\n",
      " |        variable map is created using the restored values\n",
      " |      checkpoint: str, default None\n",
      " |        the path to the checkpoint file to load.  If this is None, the most recent\n",
      " |        checkpoint will be chosen automatically.  Call get_checkpoints() to get a\n",
      " |        list of all available checkpoints\n",
      " |      model_dir: str, default None\n",
      " |        Restore model from custom model directory if needed\n",
      " |      include_top: bool, default True\n",
      " |          if True, copies the weights and bias associated with the final dense\n",
      " |          layer. Used only when assignment map is None\n",
      " |  \n",
      " |  predict(self, dataset, transformers=[], outputs=None)\n",
      " |      Uses self to make predictions on provided Dataset object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: dc.data.Dataset\n",
      " |        Dataset to make prediction on\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      outputs: Tensor or list of Tensors\n",
      " |        The outputs to return.  If this is None, the model's standard prediction\n",
      " |        outputs will be returned.  Alternatively one or more Tensors within the\n",
      " |        model may be specified, in which case the output of those Tensors will be\n",
      " |        returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a NumPy array of the model produces a single output, or a list of arrays\n",
      " |      if it produces multiple outputs\n",
      " |  \n",
      " |  predict_on_batch(self, X, transformers=[], outputs=None)\n",
      " |      Generates predictions for input samples, processing samples in a batch.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the input data, as a Numpy array.\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      outputs: Tensor or list of Tensors\n",
      " |        The outputs to return.  If this is None, the model's standard prediction\n",
      " |        outputs will be returned.  Alternatively one or more Tensors within the\n",
      " |        model may be specified, in which case the output of those Tensors will be\n",
      " |        returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a NumPy array of the model produces a single output, or a list of arrays\n",
      " |      if it produces multiple outputs\n",
      " |  \n",
      " |  predict_on_generator(self, generator, transformers=[], outputs=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generator: generator\n",
      " |        this should generate batches, each represented as a tuple of the form\n",
      " |        (inputs, labels, weights).\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      outputs: Tensor or list of Tensors\n",
      " |        The outputs to return.  If this is None, the model's standard prediction\n",
      " |        outputs will be returned.  Alternatively one or more Tensors within the\n",
      " |        model may be specified, in which case the output of those Tensors will be\n",
      " |        returned.\n",
      " |      Returns:\n",
      " |        a NumPy array of the model produces a single output, or a list of arrays\n",
      " |        if it produces multiple outputs\n",
      " |  \n",
      " |  predict_uncertainty(self, dataset, masks=50)\n",
      " |      Predict the model's outputs, along with the uncertainty in each one.\n",
      " |      \n",
      " |      The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.\n",
      " |      It involves repeating the prediction many times with different dropout masks.\n",
      " |      The prediction is computed as the average over all the predictions.  The\n",
      " |      uncertainty includes both the variation among the predicted values (epistemic\n",
      " |      uncertainty) and the model's own estimates for how well it fits the data\n",
      " |      (aleatoric uncertainty).  Not all models support uncertainty prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: dc.data.Dataset\n",
      " |        Dataset to make prediction on\n",
      " |      masks: int\n",
      " |        the number of dropout masks to average over\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      for each output, a tuple (y_pred, y_std) where y_pred is the predicted\n",
      " |      value of the output, and each element of y_std estimates the standard\n",
      " |      deviation of the corresponding element of y_pred\n",
      " |  \n",
      " |  predict_uncertainty_on_batch(self, X, masks=50)\n",
      " |      Predict the model's outputs, along with the uncertainty in each one.\n",
      " |      \n",
      " |      The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.\n",
      " |      It involves repeating the prediction many times with different dropout masks.\n",
      " |      The prediction is computed as the average over all the predictions.  The\n",
      " |      uncertainty includes both the variation among the predicted values (epistemic\n",
      " |      uncertainty) and the model's own estimates for how well it fits the data\n",
      " |      (aleatoric uncertainty).  Not all models support uncertainty prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the input data, as a Numpy array.\n",
      " |      masks: int\n",
      " |        the number of dropout masks to average over\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      for each output, a tuple (y_pred, y_std) where y_pred is the predicted\n",
      " |      value of the output, and each element of y_std estimates the standard\n",
      " |      deviation of the corresponding element of y_pred\n",
      " |  \n",
      " |  restore(self, checkpoint=None, model_dir=None, session=None)\n",
      " |      Reload the values of all variables from a checkpoint file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      checkpoint: str\n",
      " |        the path to the checkpoint file to load.  If this is None, the most recent\n",
      " |        checkpoint will be chosen automatically.  Call get_checkpoints() to get a\n",
      " |        list of all available checkpoints.\n",
      " |      model_dir: str, default None\n",
      " |        Directory to restore checkpoint from. If None, use self.model_dir.\n",
      " |      session: tf.Session(), default None\n",
      " |        Session to run restore ops under. If None, self.session is used.\n",
      " |  \n",
      " |  save_checkpoint(self, max_checkpoints_to_keep=5, model_dir=None)\n",
      " |      Save a checkpoint to disk.\n",
      " |      \n",
      " |      Usually you do not need to call this method, since fit() saves checkpoints\n",
      " |      automatically.  If you have disabled automatic checkpointing during fitting,\n",
      " |      this can be called to manually write checkpoints.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      model_dir: str, default None\n",
      " |        Model directory to save checkpoint to. If None, revert to self.model_dir\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from deepchem.models.models.Model:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  evaluate(self, dataset, metrics, transformers=[], per_task_metrics=False)\n",
      " |      Evaluates the performance of this model on specified dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: dc.data.Dataset\n",
      " |        Dataset object.\n",
      " |      metric: deepchem.metrics.Metric\n",
      " |        Evaluation metric\n",
      " |      transformers: list\n",
      " |        List of deepchem.transformers.Transformer\n",
      " |      per_task_metrics: bool\n",
      " |        If True, return per-task scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        Maps tasks to scores under metric.\n",
      " |  \n",
      " |  get_num_tasks(self)\n",
      " |      Get number of tasks.\n",
      " |  \n",
      " |  get_task_type(self)\n",
      " |      Currently models can only be classifiers or regressors.\n",
      " |  \n",
      " |  reload(self)\n",
      " |      Reload trained model from disk.\n",
      " |  \n",
      " |  save(self)\n",
      " |      Dispatcher function for saving.\n",
      " |      \n",
      " |      Each subclass is responsible for overriding this method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from deepchem.models.models.Model:\n",
      " |  \n",
      " |  get_model_filename(model_dir)\n",
      " |      Given model directory, obtain filename for the model itself.\n",
      " |  \n",
      " |  get_params_filename(model_dir)\n",
      " |      Given model directory, obtain filename for the model itself.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GraphConvModel(len(esol_labels), batch_size= 128, mode='regression')\n",
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.unsorted_segment_max is deprecated. Please use tf.math.unsorted_segment_max instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/deepchem/models/keras_model.py:169: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/deepchem/models/optimizers.py:76: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/deepchem/models/keras_model.py:258: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/deepchem/models/keras_model.py:260: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:318: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.9536899002515603]\n",
      "computed_metrics: [0.8222152933705345]\n",
      "computed_metrics: [0.8094500993708863]\n",
      "Train scores\n",
      "{'mean-pearson_r2_score': 0.9536899002515603}\n",
      "Validation scores\n",
      "{'mean-pearson_r2_score': 0.8222152933705345}\n",
      "Test scores\n",
      "{'mean-pearson_r2_score': 0.8094500993708863}\n"
     ]
    }
   ],
   "source": [
    "graph_conv(esol_labels, esol_dataset, esol_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freesolv Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from FreeSolv/SAMPL.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.549 s\n",
      "TIMING: dataset construction took 0.648 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.118 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.066 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.059 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.269 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.063 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.055 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.103 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.016 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.017 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "freesolv_labels, freesolv_dataset, freesolv_trans = data_load(['expt'], 'smiles', 'FreeSolv/SAMPL.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.9875903953321227]\n",
      "computed_metrics: [0.9021377352841167]\n",
      "computed_metrics: [0.9135036015596026]\n",
      "Train scores\n",
      "{'mean-pearson_r2_score': 0.9875903953321227}\n",
      "Validation scores\n",
      "{'mean-pearson_r2_score': 0.9021377352841167}\n",
      "Test scores\n",
      "{'mean-pearson_r2_score': 0.9135036015596026}\n"
     ]
    }
   ],
   "source": [
    "graph_conv(freesolv_labels, freesolv_dataset, freesolv_trans, batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lipophilicity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from lipophilicity/Lipophilicity.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "TIMING: featurizing shard 0 took 7.265 s\n",
      "TIMING: dataset construction took 8.707 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.779 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.618 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.844 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.519 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.870 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.640 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.463 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.156 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.173 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "lipo_labels, lipo_dataset, lipo_trans = data_load(['exp'], 'smiles', 'lipophilicity/Lipophilicity.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/abhigyaurja/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.7680606276374528]\n",
      "computed_metrics: [0.63346888706968]\n",
      "computed_metrics: [0.6531321969963345]\n",
      "Train scores\n",
      "{'mean-pearson_r2_score': 0.7680606276374528}\n",
      "Validation scores\n",
      "{'mean-pearson_r2_score': 0.63346888706968}\n",
      "Test scores\n",
      "{'mean-pearson_r2_score': 0.6531321969963345}\n"
     ]
    }
   ],
   "source": [
    "graph_conv(lipo_labels, lipo_dataset, lipo_trans, batch_size = 400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
